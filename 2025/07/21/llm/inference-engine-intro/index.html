<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM推理引擎介绍 | kangqi's blog</title><meta name="author" content="kangqi"><meta name="copyright" content="kangqi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="推理引擎1、Transformer架构（1706）Attention Is All You Need Transformer之前，主流的序列传导模型都是基于RNN（循环神经网络）和CNN（卷积神经网络），最佳性能的模型同时也基于注意力的“Encoder-Decoder”架构。Transformer提出了一种简单的网络架构，只基于注意力机制，抛弃了卷积和循环，通过两项机器翻译的实验，表明其不仅精度高">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM推理引擎介绍">
<meta property="og:url" content="https://zhaikangqi331.github.io/2025/07/21/llm/inference-engine-intro/index.html">
<meta property="og:site_name" content="kangqi&#39;s blog">
<meta property="og:description" content="推理引擎1、Transformer架构（1706）Attention Is All You Need Transformer之前，主流的序列传导模型都是基于RNN（循环神经网络）和CNN（卷积神经网络），最佳性能的模型同时也基于注意力的“Encoder-Decoder”架构。Transformer提出了一种简单的网络架构，只基于注意力机制，抛弃了卷积和循环，通过两项机器翻译的实验，表明其不仅精度高">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaikangqi331.github.io/img/touxiang.png">
<meta property="article:published_time" content="2025-07-21T08:41:55.000Z">
<meta property="article:modified_time" content="2025-07-21T09:16:05.758Z">
<meta property="article:author" content="kangqi">
<meta property="article:tag" content="inference-engine">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="ai-infra">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaikangqi331.github.io/img/touxiang.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM推理引擎介绍",
  "url": "https://zhaikangqi331.github.io/2025/07/21/llm/inference-engine-intro/",
  "image": "https://zhaikangqi331.github.io/img/touxiang.png",
  "datePublished": "2025-07-21T08:41:55.000Z",
  "dateModified": "2025-07-21T09:16:05.758Z",
  "author": [
    {
      "@type": "Person",
      "name": "kangqi",
      "url": "https://zhaikangqi331.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaikangqi331.github.io/2025/07/21/llm/inference-engine-intro/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM推理引擎介绍',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">kangqi's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM推理引擎介绍</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">LLM推理引擎介绍</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-21T08:41:55.000Z" title="发表于 2025-07-21 16:41:55">2025-07-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-21T09:16:05.758Z" title="更新于 2025-07-21 17:16:05">2025-07-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="推理引擎"><a href="#推理引擎" class="headerlink" title="推理引擎"></a>推理引擎</h1><h3 id="1、Transformer架构"><a href="#1、Transformer架构" class="headerlink" title="1、Transformer架构"></a><strong>1、Transformer架构</strong></h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762">（1706）Attention Is All You Need</a></p>
<p>Transformer之前，主流的序列传导模型都是基于RNN（循环神经网络）和CNN（卷积神经网络），最佳性能的模型同时也基于注意力的“Encoder-Decoder”架构。Transformer提出了一种简单的网络架构，只基于注意力机制，抛弃了卷积和循环，通过两项机器翻译的实验，表明其不仅精度高，而且训练时间大幅缩短。</p>
<p>Transformer架构通过证明仅凭注意力机制即可实现优于循环和卷积方法的性能，从而代表了序列建模的范式转变。它能够并行处理序列，同时在长距离依赖方面保持强大的性能，解决了先前架构的关键限制。其效率和有效性使得在更广泛的数据集上训练更大的模型成为可能，为变革自然语言处理的大语言模型奠定了基础。</p>
<p><img src="/images/llm/inference-engine-intro/image.png" alt="image.png"></p>
<p>图1.1 Transformer架构</p>
<p>Transformer保留了编码器（左半部分）和解码器（右半部分）框架，编码器由N&#x3D;6个相同层的堆叠而成，每一层包含两个子层：第一个子层为多头自注意力机制，第二个子层是一个简单的、位置相关的全连接前馈网络。在这两个子层周围，分别使用残差连接，然后进行层归一化。解码器也由N&#x3D;6个相同层的堆叠构成，除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，该层对于编码器堆栈的输出执行多头注意力机制，同时，还修改了解码器堆栈中的自注意力子层，以防止当前位置关注后续位置。这种掩码，加上输出嵌入偏移一个位置，确保了位置i的预测仅依赖于小于i位置的已知输出。</p>
<p><img src="/images/llm/inference-engine-intro/image%201.png" alt="image.png"></p>
<p>图1.2：注意力机制</p>
<ul>
<li>（左侧）缩放点积注意力 （右侧）多头注意力</li>
</ul>
<h3 id="2、大语言模型进化树"><a href="#2、大语言模型进化树" class="headerlink" title="2、大语言模型进化树"></a><strong>2、大语言模型进化树</strong></h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.13712">（2304）Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></p>
<p><img src="/images/llm/inference-engine-intro/image%202.png" alt="image.png"></p>
<p>图2.1 大语言模型进化树</p>
<p>基于Transformer的模型以非灰色表示：</p>
<ul>
<li>仅解码器模型：右侧蓝色分支（GPT、LlaMA、DeepSeek、Claude）</li>
<li>编码器-解码器模型：中间绿色分支</li>
<li>仅编码器模型：左侧红色分支</li>
<li>开源模型：实心方块；闭源模型：空心方块</li>
</ul>
<p>Encoder-decoder模型首先对整个输入进行编码，然后在每一步使用带有交叉注意力的解码器，这会导致更高的内存占用和更复杂的推理过程。仅编码器模型适用于分类或检索等任务，但由于它们是为一次性推理优化的，因此并不适合逐个词生成。相比之下，仅解码器模型结构更为简单，因其通过自回归训练展现出的强大零样本性能，近年来被大语言模型广泛采用。</p>
<p>Encoder-Only：分类、检索等理解类任务</p>
<p>Encoder-Decoder：机器翻译、文本摘要等输入输出配对的任务</p>
<p>Decoder-Only：文本生成、自动补全等生成类任务</p>
<h3 id="3、大模型推理流程"><a href="#3、大模型推理流程" class="headerlink" title="3、大模型推理流程"></a><strong>3、大模型推理流程</strong></h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.01658">（2505）A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency</a></p>
<p><img src="/images/llm/inference-engine-intro/image%203.png" alt="image.png"></p>
<p>图3.1 仅解码器架构</p>
<ul>
<li>接收输入文本-&gt;分词-&gt;向量化-&gt;位置编码-&gt;Transformer块运算-&gt;归一化-&gt;线性层-&gt;概率分布转换-&gt;输出最高概率token</li>
<li>完成transformer块运算之后，每个输入token会生成隐藏状态，经归一化后用于文本生成中的下一token预测。隐藏状态通过线性层转换为词表上的逻辑值向量，经Softmax函数转为概率分布后，选择最高概率token作为输出。该过程循环执行直至生成完整文本。</li>
<li>每个Transformer块包含多头注意力（MHA）、前馈网络（FFN）和残差连接。</li>
<li>MHA层将输入拆分为查询（Q）、键（K）和值（V）向量，并在多个注意力头上并行执行缩放点积注意力计算。每个头内会计算Q-K相似度分数并应用于V向量，最终聚合结果。因果掩码确保仅关注已生成的token，从而实现自回归上下文学习。</li>
<li>随后，FFN层通过线性变换对注意力输出进行细化：先将其映射到更高维空间并应用激活函数（如ReLU、GELU或SiLU），再降维回原始尺寸。这一操作序列增强了模型的表征能力。</li>
<li>MHA层和FFN层均采用残差连接和层归一化。残差连接缓解了深层网络的梯度消失问题，层归一化则稳定输出分布以优化训练过程。</li>
</ul>
<p><img src="/images/llm/inference-engine-intro/image%204.png" alt="image.png"></p>
<p>图3.2 大模型推理流程</p>
<p>大语言模型的推理过程首先对用户输入文本进行分词，随后持续生成后续token直至满足停止条件（如达到token数量上限或遇到序列结束符【EOS】），该过程包含两个核心阶段：预填充和解码。</p>
<p><strong>Prefill Phase（预填充阶段-基于输入生成首个token）</strong></p>
<p>本阶段通过处理输入文本来计算每个样本最后一个词元的隐藏状态，从而捕捉输入的上下文和语义信息。在仅解码器Transformer模型中，该阶段需执行分词、嵌入和Transformer模块处理等核心步骤，所有输入tokens都要经过注意力和FFN双重处理：</p>
<ul>
<li>计算复杂度：注意力机制的计算量随序列长度n成平方级增长（O(n²)）,前馈网络FFN的复杂度则与中间层维度相关，二者均涉及大规模矩阵运算。</li>
<li>数据加载：用于捕获所有输入tokens间关系的Q&#x2F;K&#x2F;V矩阵会即时生成，需要将海量数据载入内存进行密集计算。</li>
</ul>
<p>例如：输入问题”Is Seoul in Korea?”被分词为[Is, Seoul, in, Korea, ?]，映射为token ID如[101,4523,1102,2342,63]。经位置编码转换为高维向量（如[[0.1,0.2,…],[0.3,0.5,…],…]），通过注意力机制和FFN运算学习token间上下文关系并优化表征。最终存储最后一个token(?)的隐藏状态，用于指导解码阶段生成后续内容。</p>
<p><strong>Decode Phase（解码阶段-顺序生成剩余tokens）</strong></p>
<p>本阶段基于预填充阶段计算的隐藏状态，通过自回归机制迭代生成新token，在此过程中，每次仅生成单个token。这个阶段，transformer模块的最终隐藏状态经过线性变换和softmax函数，生成词汇表的概率分布，选择概率最高的token并将其追加到输入序列。在这个过程中，K、V以及输入输出tokens都被保存在GPU、系统内存或缓存中，由于K和V必须被频繁的获取更新，解码阶段内存带宽常常成为关键瓶颈。尽管注意力计算类似于预填充阶段，但频繁引用先前生成的tokens会增加延迟，并且要访问的数据随序列长度线性增长。</p>
<p>接着上面的例子，在解码阶段，预填充阶段保存的最后一个token(?)的隐藏状态被用于预测下一个token: “Yes”，然后重复transformer块的计算，直到生成完整的响应。</p>
<h3 id="4、大模型推理优化方法"><a href="#4、大模型推理优化方法" class="headerlink" title="4、大模型推理优化方法"></a><strong>4、大模型推理优化方法</strong></h3><p><strong>4.1 关键指标</strong></p>
<table>
<thead>
<tr>
<th>衡量指标</th>
<th>定义</th>
<th>用户视角</th>
<th>优化技术</th>
</tr>
</thead>
<tbody><tr>
<td>Time-to-first-token</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(TTFT)</td>
<td>生成第一个token的耗时</td>
<td>影响用户对响应速度的感知</td>
<td></td>
</tr>
<tr>
<td>Time-between-tokens</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(TBT)</td>
<td>每个token之间的耗时</td>
<td>反应token的生成速度</td>
<td></td>
</tr>
<tr>
<td>End-to-end latency</td>
<td>请求到完成相应的总耗时</td>
<td>反映了总体响应时间和用户体验</td>
<td></td>
</tr>
<tr>
<td>Throughput</td>
<td>单位时间内处理的tokens数量</td>
<td>表示系统的处理能力</td>
<td></td>
</tr>
</tbody></table>
<p>表4.1 大模型性能关键指标</p>
<p>从阶段划分的角度来看，预填充阶段会影响TTFT、解码阶段会影响TBT。。预填充阶段的延迟会随着输入的增长而增长，但通过并行计算可以有效降低。另一方面，解码阶段的延迟会随着生成的token数量增多而增长，对用户体验的影响更为直接。</p>
<p><strong>4.2 优化方法</strong></p>
<p>考虑到这些性能指标，大模型推理引擎在预填充和解码阶段采用了多种定制化优化技术。大多数引擎使用KV缓存（KV-Cache），以在解码过程中避免冗余计算，通过复用缓存的上下文，仅对最新的token进行新的计算。近年来还引入了持续批处理（continuous batching）和混合批处理（hybrid batching），以进一步提升解码阶段的效率，将来自多个请求的预填充和解码操作进行分组，从而更好地利用GPU资源。</p>
<p>此外，许多推理引擎通过内核融合（kernel fusion）和硬件特定的计算内核来减少解码时，每个token的开销。内核融合将诸如层归一化（LayerNorm）、矩阵乘法和激活函数等操作整合到单一的GPU内核中，从而减少内存访问和内核启动的开销。</p>
<p>量化也是一项关键优化。通过将模型参数从16位或32位浮点数表示为8位或4位整数，可以显著降低内存使用和带宽需求，尤其是在解码阶段。量化后的模型能够缓存更多的token，并在同一硬件上处理更多并发请求，通常还能提升计算速度。</p>
<p>总体而言，缓存(caching)、批处理(batching)、内核优化(kernel optimization)和量化(quantization)是优化token吞吐量、减少推理延迟的基础技术。推理引擎对这些技术的强力支持，是实现高质量、可扩展的大模型推理服务的关键。</p>
<p>图4.2 大模型推理优化方法分类</p>
<p>大语言模型推理性能不仅取决于模型规模和硬件环境，还与多种推理优化技术密切相关。以下是各类型优化技术的原理介绍：</p>
<p><strong>4.2.1 批量优化</strong></p>
<p>在LLM 推理中，批量处理将多个输入请求组合在一起进行同步处理，从而提高硬件利用率和吞吐量，高效的批处理机制对于最大化计算并行性和延迟至关重要。</p>
<p>因此，确定批量大小至关重要：小批量能缩短响应时间，但可能造成硬件资源利用率不足；大批量虽然能提升吞吐量，却可能延长响应时间。推理引擎通常会提供机制，根据工作负载和SLOs来探索最佳批量规模。</p>
<p>除了批量大小之外，调度方法也显著影响推理性能。静态批处理会固定处理一定数量的请求，但新请求必须等待批处理完成，可能导致延迟增加。相比之下，动态批处理和持续批处理能够实时调整批处理规模，通常能够有效降低延迟并提升整体效率。</p>
<p><img src="/images/llm/inference-engine-intro/image%205.png" alt="image.png"></p>
<p>图4.2.1 批处理策略对比</p>
<p>动态批处理（Dynamic Batching）：缓解了静态批处理的延迟和硬件利用率低的问题。新的请求会被立即加入进行中的批次中，从而实现更灵活高效的推理。动态批处理会基于传入的请求和可用的硬件资源重新构建批处理，并自适应的确定批次大小。当新的请求到达时，它可以与现有的批次合并或者加入正在进行的流程中，以优化资源利用率。实现动态批处理需要优化几个参数：最大批等待时间、最小批大小、最大批大小。如果请求输入的提示词或者输出的tokens长度差异很大，动态批处理也可能引入动态调整批次大小的开销而降低性能。</p>
<p>持续批处理（Continuous Batching）：类似于动态批处理，但是允许新的请求不中断地加入正在进行的批次中，减少延迟。请求会持续的插入，最大化GPU和显存利用率。持续批处理需要复杂的调度、高效的KV缓存管理（经常使用PagedAttention），该方案来自于Orca。</p>
<p><img src="/images/llm/inference-engine-intro/image%206.png" alt="image.png"></p>
<p>图4.2.1.1 分块预填充</p>
<p>分块预填充（chunked-prefills）：为了解决动态或持续批推理流程中的流水线低效问题，尤其是多GPU环境下：当有很长的提示词（promt），会拖长预填充时间，解码阶段可能会限制，还会拖慢较短请求的返回。通过把长提示词拆块，让预填充与解码重叠执行，以更高GPU利用率换取更低的整体延迟，但需要更精细的调度与显存管理。</p>
<p><strong>4.2.2</strong> <strong>并行</strong></p>
<p>大模型参数过大时，单卡放不下，需要多卡多节点分布式并行处理。可用的GPU数量、互联带宽、内存层级结构、算力等因素都会影响张量并行（Tensor Parallslism）、数据并行（Data Parallelism）、全分片数据并行（FSDP）和流水线并行（Pipline Parallelism）等策略效果，混合并行策略也能进一步提升性能。例如在多节点场景，需要优化通信延迟，可以使用通信压缩或异步调度等技术。单节点多卡架构中，依赖共享内存和高速互联（如NVLink、NVSwitch）。总之、LLM推理的并行策略需要根据具体的硬件环境灵活组合，兼顾性能与效率。</p>
<p><img src="/images/llm/inference-engine-intro/image%207.png" alt="image.png"></p>
<p>图4.2.2 多种并行策略对比</p>
<p><strong>数据并行</strong>（Data Parallelism）：在多卡或多节点上复制相同的模型，一个小批次被划分给不同的设备，每个设备独立执行推理。完成计算后，输出被汇总到一个设备上以生成最终结果。这个方法实现简单，通信开销也低，但如果模型过大，无法完整加载到每个设备上时，将不适用。此外如果设备性能差异显著，系统整体可能会出现性能瓶颈。</p>
<p><strong>全分片数据并行</strong>（FSDP）：与传统的数据并行不同，FSDP 不再让每个设备持有模型完整副本和优化器状态，而是将参数、梯度和优化器状态在多个设备间切片（shard），从而避免重复内存占用，并允许使用相同硬件资源训练更大的模型。FSDP 会在每层执行前将该层的所有参数聚集到当前 GPU 上进行完整计算，执行结束后立即释放。这种方式不拆分计算操作，因此易于实现并兼容大多数模型。但由于每层都需要聚合参数，这会引入通信开销，特别在小批量或低延迟场景中影响性能。此外，若某层需要的聚合参数超过单个 GPU 内存限制，该层就无法运行。</p>
<p>训练过程中，FSDP 可通过 shard 激活和参数显著节省内存。但推理过程中没有梯度或激活反向传播，因此节省较少。是否在推理中使用 FSDP，应视模型大小而定。FSDP 是 PyTorch 原生支持的，能很好结合其自动求导、checkpoint 和混合精度训练，并可与其他并行方法（如混合并行）灵活组合。vLLM、SGLang支持FSDP。</p>
<p><strong>张量并行</strong>（TP）：将特定的 LLM 操作（如矩阵乘法、注意力机制、全连接层）划分到多个设备上执行。每个设备处理其中一部分，之后通过通信汇总中间结果。假设 4 张 GPU 共同处理矩阵乘法X×A&#x3D;Y，矩阵 A 可以按行或列划分到各个 GPU 上，最终通过 All-Reduce 或 All-Gather 汇总。张量并行能分摊大计算任务，加快推理速度，减少单卡内存压力（因为每张卡不需存全部权重）。但频繁的跨设备通信可能增加开销，不当的分片策略也会导致效率下降。vLLM、SGLang都支持。</p>
<p><strong>流水线并行</strong>（PP）：将模型的不同部分（如 Transformer 层）分配到不同 GPU 上，输入数据被划分成 micro-batch，依次通过层级组成的“流水线”。vLLM、SGLang都支持。</p>
<p><strong>4.2.3</strong> <strong>压缩</strong></p>
<p>**量化（Quantization）：**量化是一种将预训练的 FP32 或 FP16 模型转换为更低精度格式（如 FP4、FP8、INT4、INT8）的技术，可以显著减少内存消耗并提升推理速度，仅带来较小精度损失。</p>
<p>**剪枝（Pruning）：**剪枝通过移除不重要的模型参数来压缩模型、提升效率。</p>
<p>**稀疏优化（Sparsity Optimization）：**稀疏优化通过增加模型中零值数量来降低计算开销。与剪枝类似目标，但更注重模型结构或计算模式设计。</p>
<table>
<thead>
<tr>
<th>压缩技术</th>
<th>核心作用</th>
<th>优点</th>
<th>挑战</th>
<th>代表方法</th>
</tr>
</thead>
<tbody><tr>
<td><strong>量化</strong></td>
<td>低精度表示模型参数</td>
<td>降低显存、提升速度</td>
<td>精度下降、需量化内核支持</td>
<td>GPTQ、AWQ、SmoothQuant、KVQuant、Marlin</td>
</tr>
<tr>
<td><strong>剪枝</strong></td>
<td>移除不重要参数</td>
<td>降低模型大小、提升稀疏性</td>
<td>稀疏计算需内核支持、图结构变化</td>
<td>Wanda、LLM-Pruner、Mini-GPTs</td>
</tr>
<tr>
<td><strong>稀疏优化</strong></td>
<td>增加零值以减少计算</td>
<td>提升效率、降低功耗</td>
<td>稀疏模式需硬件支持</td>
<td>N:M 稀疏、MoE、Deja Vu、xFormers</td>
</tr>
</tbody></table>
<p><strong>4.2.4</strong> <strong>缓存</strong></p>
<p>对于包含数十亿甚至上万亿参数的 LLM，在生成数百至数百万个 token 的过程中需要大量的计算和内存资源。为解决这一问题，大多数推理引擎采用了多种缓存策略，以减少冗余计算、降低延迟。缓存机制可应用于多个 LLM 组件，并可组合优化使用。</p>
<p>**提示缓存（Promt Caching）：**大量 LLM 的 prompt 会重复包含相似或相同的文本，例如系统消息、说明文字等，尤其是在对话机器人、编程助手或长文档处理中尤为常见。通过预存常用文本片段的注意力状态（attention states），在遇到相同片段时重复利用这些计算结果，仅处理新内容，从而提升推理效率。由于 Transformer 架构使用位置编码（positional encoding），这些缓存只能在片段出现在相同位置时复用。为解决位置依赖问题，提出了：Prompt Markup Language (PML)：定义提示结构，标记可复用片段（prompt module），为每个模块分配唯一的位置 ID，实现在模块级别的 attention 缓存复用。</p>
<p>**前缀缓存（Prefix Caching）：**Prefix Caching 与 Prompt Caching 类似，但更精细地只缓存 prompt 中重复出现的前缀部分。</p>
<ul>
<li>适用于 <strong>批量推理场景</strong>：多个请求共享相同前缀（如 few-shot 示例、系统说明），可以重用前缀部分的计算，减少预填阶段的开销。</li>
<li>仅加速 <strong>预填阶段</strong>，不加速解码（decode）阶段。</li>
<li>如果请求不共享前缀，则无法命中缓存。</li>
</ul>
<p>**键值缓存（KV Caching）：**Transformer 中每个 token 都需要关注前面所有 token，时间复杂度为O(n²)。KV 缓存（Key-Value Caching） 可将复杂度降至 O(n)：</p>
<ul>
<li>在每个 token 步中存储注意力机制中的 Key 和 Value 矩阵；</li>
<li>之后生成 token 时，重用历史 token 的 K&#x2F;V，避免重复计算；</li>
<li>广泛应用于多轮对话、长上下文推理中，能与前缀缓存、推测解码（speculative decoding）等优化手段协同使用。</li>
</ul>
<p><img src="/images/llm/inference-engine-intro/image%208.png" alt="image.png"></p>
<p>图4.2.4.1 KV caching</p>
<p><strong>4.2.5</strong> <strong>Attention 优化</strong></p>
<p>Transformer 架构中的 Attention 是 LLM 的核心模块，但其计算和内存开销随序列长度二次增长。因此，大模型推理引擎在部署时必须优化 Attention 机制，以降低延迟、减少显存占用，并提升吞吐量。</p>
<p>**KV Cache 优化：**传统 KV 缓存采用连续内存分配，导致内&#x2F;外碎片化，影响内存效率和并发能力。</p>
<ul>
<li><strong>PagedAttetion（分页注意力）</strong>：<ul>
<li>类似 Linux 内存分页机制，将 KV 缓存分为小块，并使用页表进行映射；<ul>
<li>实现按需分配、快速回收、跨请求共享 KV。</li>
</ul>
</li>
</ul>
</li>
<li>支持：vLLM、DeepSpeed-FastGen、MAX、SGLang。</li>
<li><strong>TokenAttention（LightLLM 提出）</strong>：</li>
<li>用 token 表替代页表，按 token 粒度追踪 KV 缓存位置。</li>
<li>支持更精细的内存管理。</li>
<li><strong>ChunkedAttention（块化注意力）</strong>：</li>
<li>将重复的系统提示缓存成共享块（如 Prefix-Aware KV Cache）。</li>
<li>提高缓存重用率和内存效率。</li>
</ul>
<p><strong>IO优化</strong>：Attention 计算涉及大量 GPU 层级间的数据搬运，内存带宽瓶颈明显。</p>
<ul>
<li><strong>FlashAttention 技术</strong>：</li>
<li>切分 Q&#x2F;K&#x2F;V 矩阵为 tile，并在每 tile 上执行 softmax，避免频繁写回。</li>
<li>融合了矩阵乘法 + softmax 为单个 kernel。</li>
<li>显著降低内存占用并支持长序列。</li>
</ul>
<p><strong>KV Cache复用：</strong></p>
<ul>
<li><strong>RadixAttention（SGLang）：</strong></li>
<li>使 用 Radix Tree + LRU 机制管理 KV 缓存。</li>
<li>多个请求共享 KV 块。</li>
<li>支持跨批次、跨 GPU KV 缓存复用。</li>
<li>Meta-Tree 管理所有 GPU 子树结构，实现高效路由与缓存更新。</li>
</ul>
<p><strong>4.2.5</strong> <strong>采样优化</strong></p>
<p><strong>推测解码（speculative decoding）</strong>：借鉴CPU中的“推测执行”，使用一个轻量草稿模型（draft model）预测多个token，主模型（target model）对其并行验证，快速推进生成过程。</p>
<p>推测解码的优化：EAGLE 系列（google提出）：</p>
<ul>
<li>EAGLE：使用“树结构”草稿模型，仅加一层 Transformer 解码器，代价小<ul>
<li>EAGLE-2 利用置信度预测动态调整树结构，提高接受率</li>
<li>EAGLE-3 移除原有特征限制，直接预测 token，支持多阶段生成，速度更快</li>
</ul>
</li>
</ul>
<p>模型支持：</p>
<ul>
<li>vLLM：支持 offline speculative decoding，可一次预测最多 5 个 token，使用 n-gram 相似度判断<ul>
<li>SGLang：支持 EAGLE1&#x2F;2&#x2F;3，与 Radix Cache、chunked prefill 深度整合</li>
</ul>
</li>
</ul>
<p><strong>4.2.6 结构化输出</strong></p>
<p>结构化输出指生成符合预定义格式（如 JSON、XML 等）的文本，确保生成结果满足下游系统的要求，比如数据库写入、API 调用等。与自由文本生成不同，结构化生成提升了结果的正确性、一致性，便于解析和集成，还可减少幻觉（无效&#x2F;错误内容），通常不需要额外微调即可满足领域需求。</p>
<p><strong>4.2.7</strong> <strong>微调</strong></p>
<p>大语言模型通常依赖在大规模数据集上预训练的基础模型来完成各种推理任务。然而，为了适应特定领域或任务，微调可以显著提升模型性能。微调最初应用于卷积神经网络（CNN），用于修改预训练模型的参数。微调分为两类：</p>
<ul>
<li>全量参数微调（full-parameter finetuning）：更新所有参数；</li>
<li>参数高效微调（PEFT）：仅调整部分参数。</li>
</ul>
<p>由于全量微调对硬件资源要求极高，大语言模型更倾向于采用仅更新部分参数的 PEFT 方法。虽然微调主要用于训练阶段，但它对 LLM 的推理性能也有直接影响。</p>
<p>LoRA 是典型的 PEFT 方法。它不更新原始模型权重，而是冻结这些权重，通过训练附加的低秩矩阵来调整模型参数。</p>
<h3 id="5、大模型推理引擎对比"><a href="#5、大模型推理引擎对比" class="headerlink" title="5、大模型推理引擎对比"></a><strong>5、大模型推理引擎对比</strong></h3><table>
<thead>
<tr>
<th>推理引擎</th>
<th>star数</th>
<th>大厂背书</th>
<th>开发团队</th>
<th>国产适配</th>
<th>性能评测</th>
</tr>
</thead>
<tbody><tr>
<td>vLLM</td>
<td>52.5k</td>
<td></td>
<td>加州大学伯克利分校UCB</td>
<td>支持Ascend</td>
<td></td>
</tr>
<tr>
<td>SGLang</td>
<td>16.1k</td>
<td>xAI、AMD、Microsoft Azure、美团等</td>
<td>UCB、上海交大，主要成员都是华人</td>
<td>支持Ascend</td>
<td></td>
</tr>
<tr>
<td>LMDeploy</td>
<td>6.7k</td>
<td>ModelScope Swift的vlms默认推理引擎，VLMs的事实标准</td>
<td>上海AI Lab</td>
<td>支持Ascend</td>
<td></td>
</tr>
<tr>
<td>MindIE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>下面从多个角度比较常用的推理引擎：vLLM、SGLang、LMDeploy等</p>
<p><img src="/images/llm/inference-engine-intro/image%209.png" alt="image.png"></p>
<p><img src="/images/llm/inference-engine-intro/image%2010.png" alt="image.png"></p>
<p><img src="/images/llm/inference-engine-intro/image%2011.png" alt="image.png"></p>
<p>图5.1 基于6个维度的特征比较：模型通用性、易于部署和使用、延迟和吞吐量优化、可扩展性</p>
<p><img src="/images/llm/inference-engine-intro/image%2012.png" alt="image.png"></p>
<p>图5.2 基于是否开源、star数、支持的模型数、文档、社区等</p>
<p><img src="/images/llm/inference-engine-intro/image%2013.png" alt="image.png"></p>
<p>图5.3 后端硬件支持</p>
<p><img src="/images/llm/inference-engine-intro/image%2014.png" alt="image.png"></p>
<p>图5.3.1 按分布式和异构硬件支持划分的四象限</p>
<p><img src="/images/llm/inference-engine-intro/image%2015.png" alt="image.png"></p>
<p>图5.4 优化方法</p>
<p><img src="/images/llm/inference-engine-intro/image%2016.png" alt="image.png"></p>
<p>图5.5 支持的数据类型</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaikangqi331.github.io">kangqi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaikangqi331.github.io/2025/07/21/llm/inference-engine-intro/">https://zhaikangqi331.github.io/2025/07/21/llm/inference-engine-intro/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://zhaikangqi331.github.io" target="_blank">kangqi's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/inference-engine/">inference-engine</a><a class="post-meta__tags" href="/tags/llm/">llm</a><a class="post-meta__tags" href="/tags/ai-infra/">ai-infra</a></div><div class="post-share"><div class="social-share" data-image="/img/touxiang.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/03/26/dcgm/dcgm-exporter-profile/" title="dcgm-exporter profile指标采集错误"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">dcgm-exporter profile指标采集错误</div></div><div class="info-2"><div class="info-item-1">1、dcgm-exporter 采集profile指标，启动报错： 1Error watching fields: The third-party Profiling module returned an unrecoverable error  2、尝试在node节点上运行 dcgmi dmon -e 1002 ，报同样的错误，但是在启动dcgm-exporter之前是可以的。 3、查看节点的&#x2F;var&#x2F;log&#x2F;nv-hostengine.log日志： 123452025-03-26 15:46:55.442 ERROR [61667:62314] [[Profiling]] [PerfWorks] Got status 20 from NVPW_DCGM_PeriodicSampler_BeginSession() on deviceIndex 0 [/workspaces/dcgm-rel_dcgm_3_3-postmerge/dcgm_private/modules/profiling/DcgmLopGpu.cpp:351] [DcgmLopG...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">kangqi</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E"><span class="toc-number">1.</span> <span class="toc-text">推理引擎</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81Transformer%E6%9E%B6%E6%9E%84"><span class="toc-number">1.0.1.</span> <span class="toc-text">1、Transformer架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E5%8C%96%E6%A0%91"><span class="toc-number">1.0.2.</span> <span class="toc-text">2、大语言模型进化树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">1.0.3.</span> <span class="toc-text">3、大模型推理流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.0.4.</span> <span class="toc-text">4、大模型推理优化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E5%AF%B9%E6%AF%94"><span class="toc-number">1.0.5.</span> <span class="toc-text">5、大模型推理引擎对比</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/21/llm/inference-engine-intro/" title="LLM推理引擎介绍">LLM推理引擎介绍</a><time datetime="2025-07-21T08:41:55.000Z" title="发表于 2025-07-21 16:41:55">2025-07-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/26/dcgm/dcgm-exporter-profile/" title="dcgm-exporter profile指标采集错误">dcgm-exporter profile指标采集错误</a><time datetime="2025-03-26T09:22:47.000Z" title="发表于 2025-03-26 17:22:47">2025-03-26</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By kangqi</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>